{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the completeness function for all stars in my sample\n",
    "I am going to try a somewhat more opaque machine-learning model to find the detection rate as a function of primary and secondary temperature, and primary and secondary vsini. We will probably have to treat the different instruments separately, but just in that there will be different best-fit parameters.\n",
    "\n",
    "## Update:\n",
    "The machine learning model works alright for predicting what kinds of companions you can detect for a new star. However, it does poorly in the interface region where the completeness drops from 1-->0. So, I ended up using 2D interpolation to get the completeness surfaces for each of my stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kgullikson/anaconda3/envs/python2/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from matplotlib.cm import viridis\n",
    "import sqlite3\n",
    "from astropy.visualization import hist\n",
    "import astropy.stats\n",
    "from astropy.modeling import models, fitting\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from scipy.interpolate import CloughTocher2DInterpolator\n",
    "import warnings\n",
    "\n",
    "sns.set_context('talk', font_scale=1.5)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "home = os.environ['HOME']\n",
    "basedir = os.path.join(home, 'School', 'Research')\n",
    "\n",
    "inst_dirs = dict(TS23=os.path.join(basedir, 'McDonaldData'),\n",
    "                 HRS=os.path.join(basedir, 'HET_data'),\n",
    "                 CHIRON=os.path.join(basedir, 'CHIRON_data'),\n",
    "                 IGRINS=os.path.join(basedir, 'IGRINS_data'))\n",
    "\n",
    "# Get the observing stats (especially the S/N)\n",
    "obs_stats = pd.read_csv('data/SampleObservations.csv')\n",
    "\n",
    "# Get the binarity/multiplicity of the stars\n",
    "with sqlite3.connect('{}/.PythonModules/Stellar_database/Stars.sqlite'.format(os.environ['HOME'])) as db_con:\n",
    "    sql_query = \"\"\"\n",
    "                   SELECT name, binarity.wds_binary, binarity.sb9_binary, binarity.my_binary, binarity.separation, binarity.K1 \n",
    "                   FROM star\n",
    "                   INNER JOIN binarity\n",
    "                   ON star.id=binarity.star_id;\n",
    "                \"\"\"\n",
    "    binarity = pd.read_sql_query(sql_query, db_con)\n",
    "binarity['known_binary'] = ((binarity.wds_binary) | (binarity.sb9_binary) | (binarity.my_binary)).astype(bool)\n",
    "binary_stars = binarity.loc[binarity.known_binary, 'name'].values\n",
    "\n",
    "def get_summary(instrument, reject_binary=False):\n",
    "    summary = pd.read_csv(os.path.join(inst_dirs[instrument], 'Sensitivity_Summary.csv'), index_col=0)\n",
    "    if reject_binary:\n",
    "        summary = summary.loc[~summary.star.isin(binary_stars)]\n",
    "    ccf_data = pd.read_csv(os.path.join(inst_dirs[instrument], 'Cross_correlations', 'CCF_primary_20151129.rv.txt'))\n",
    "    ccf_data.rename(columns=dict(teff='pri_teff', logg='pri_logg', vsini='pri_vsini'), inplace=True)\n",
    "    tmp = pd.merge(summary, ccf_data[['star', 'date', 'pri_teff', 'pri_logg', 'pri_vsini']], on=['star', 'date'], how='left')\n",
    "    return pd.merge(tmp, obs_stats.loc[obs_stats.instrument == instrument, ['star', 'date', 'exptime', 'snr']], \n",
    "                    on=('star', 'date'), how='left')\n",
    "     \n",
    "def get_injection_results(instrument, reject_binary=False):\n",
    "    results = pd.read_csv(os.path.join(inst_dirs[instrument], 'Sensitivity_Dataframe.csv'), index_col=0)\n",
    "    if reject_binary:\n",
    "        results = results.loc[~results.star.isin(binary_stars)]\n",
    "    ccf_data = pd.read_csv(os.path.join(inst_dirs[instrument], 'Cross_correlations', 'CCF_primary_20151129.rv.txt'))\n",
    "    ccf_data.rename(columns=dict(teff='pri_teff', logg='pri_logg', vsini='pri_vsini'), inplace=True)\n",
    "    tmp = pd.merge(results, ccf_data[['star', 'date', 'pri_teff', 'pri_logg', 'pri_vsini']], on=['star', 'date'], how='left')\n",
    "    return pd.merge(tmp, obs_stats.loc[obs_stats.instrument == instrument, ['star', 'date', 'exptime', 'snr']], \n",
    "                    on=('star', 'date'), how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use CHIRON data for display and testing purposes (it has the most data)\n",
    "df = get_injection_results('CHIRON', reject_binary=True)\n",
    "summary = get_summary('CHIRON', reject_binary=True)\n",
    "df['Detected'] = df.significance.notnull().astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the importance of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define features\n",
    "features = ['pri_teff', 'temperature', 'pri_vsini', 'vsini', 'snr']\n",
    "X = df.loc[df.addmode == 'simple', features].as_matrix()\n",
    "y = df.loc[df.addmode == 'simple', 'Detected'].values\n",
    "\n",
    "# Split into a training and test sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the estimator\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Find the score on the test set\n",
    "print('Classifier score = {:.2f}'.format(rf.score(X_test, y_test)))\n",
    "\n",
    "# Show feature importance\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print('')\n",
    "for f in range(X_train.shape[1]):\n",
    "    print('{}: {} +/- {}'.format(features[indices[f]], importances[indices[f]], std[indices[f]]))\n",
    "    \n",
    "# Make a plot showing the feature importances\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,7))\n",
    "ax.bar(range(X_train.shape[1]), importances[indices], color='r', yerr=std[indices], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), [features[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same thing with a pipeline\n",
    "\n",
    "I will actually use logistic regression instead of the random forest, but they are similar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validate = True\n",
    "classifier = 'SGD'\n",
    "\n",
    "# Define features\n",
    "features = ['temperature', 'pri_teff', 'vsini', 'pri_vsini', 'snr']\n",
    "X = df.loc[df.addmode == 'simple', features].as_matrix()\n",
    "y = df.loc[df.addmode == 'simple', 'Detected'].values\n",
    "\n",
    "# Split into a training and test sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the estimator\n",
    "if classifier == 'SGD':\n",
    "    clf = SGDClassifier(loss='log', alpha=0.001)\n",
    "elif classifier == 'RF':\n",
    "    clf = RandomForestClassifier(n_estimators=150, max_depth=10)\n",
    "#clf = LogisticRegressionCV(Cs=20)\n",
    "#clf = LinearSVC()\n",
    "\n",
    "clf = CalibratedClassifierCV(clf, method='isotonic')\n",
    "steps = [('Normalize', StandardScaler()), ('poly', PolynomialFeatures(2)), ('clf', clf)]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "if cross_validate:\n",
    "    # Cross-validate the SGD alpha parameter\n",
    "    mean_score = []\n",
    "    std_score = []\n",
    "    if classifier == 'SGD':\n",
    "        alphas = np.logspace(-6, 0, 10)\n",
    "    elif classifier == 'RF':\n",
    "        alphas = np.linspace(1, 300, 10, dtype=np.int)\n",
    "    for alpha in alphas:\n",
    "        print('alpha = {}'.format(alpha))\n",
    "        if classifier == 'SGD':\n",
    "            pipe.set_params(clf__base_estimator__alpha=alpha)\n",
    "        else:\n",
    "            pipe.set_params(clf__base_estimator__max_depth=alpha)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=8, n_jobs=-1)\n",
    "        mean_score.append(np.mean(scores))\n",
    "        std_score.append(np.std(scores))\n",
    "\n",
    "    plt.errorbar(alphas, mean_score, yerr=std_score)\n",
    "    if classifier == 'SGD':\n",
    "        ax = plt.gca()\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "    # Set to the best value\n",
    "    best_alpha = alphas[np.argmax(mean_score)]\n",
    "    print('Setting alpha = {}'.format(best_alpha))\n",
    "    if classifier == 'SGD':\n",
    "        pipe.set_params(clf__base_estimator__alpha=best_alpha)\n",
    "    elif classifier == 'RF':\n",
    "        pipe.set_params(clf__base_estimator__max_depth=best_alpha)\n",
    "    \n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Find the score on the test set\n",
    "print('Classifier score = {:.2f}'.format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipe.set_params(clf__base_estimator__alpha=0.01)\n",
    "X_summary = summary.loc[summary.addmode == 'simple', features]\n",
    "if hasattr(pipe, \"predict_proba\"):\n",
    "    prediction = pipe.predict_proba(X_summary)[:, 1]\n",
    "else:  # use decision function\n",
    "    prediction = pipe.decision_function(X_summary)\n",
    "    prediction = \\\n",
    "                (prediction - prediction.min()) / (prediction.max() - prediction.min())\n",
    "        \n",
    "summary.loc[summary.addmode == 'simple', 'Predicted_detrate'] = prediction\n",
    "sns.regplot('detrate', 'Predicted_detrate', data=summary, scatter_kws=dict(alpha=0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_pos = pipe.predict_proba(X_test)[:, 1]\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "            calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "plt.plot(mean_predicted_value, fraction_of_positives)\n",
    "plt.xlabel('Mean Predicted Value')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.plot((0, 1), (0, 1), 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fit to the full dataset\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the fit to some stars and plot the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_residuals(star, date, clf, features, addmode='simple', refit=False, **kwargs):\n",
    "    if refit:\n",
    "        #clf = clf.copy()\n",
    "        subset = df.loc[(df.star == star) & (df.date == date) & (df.addmode == addmode)].copy()\n",
    "        X = subset[features].as_matrix()\n",
    "        y = subset['Detected'].values\n",
    "        clf.fit(X, y)\n",
    "        \n",
    "    # Get predictions and residuals\n",
    "    subset = summary.loc[(summary.star == star) & (summary.date == date) & (summary.addmode == addmode)].copy()\n",
    "    X_summary = subset[features].as_matrix()\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prediction = clf.predict_proba(X_summary)[:, 1]\n",
    "    else:  # use decision function\n",
    "        prediction = clf.decision_function(X_summary)\n",
    "        prediction = \\\n",
    "                    (prediction - prediction.min()) / (prediction.max() - prediction.min())\n",
    "    subset['prediction'] = prediction\n",
    "    subset['diff'] = subset.detrate - subset.prediction\n",
    "    \n",
    "    # make figure\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    left = plt.subplot2grid((3, 2), (0, 0), rowspan=2)\n",
    "    right = plt.subplot2grid((3, 2), (0, 1), rowspan=2)\n",
    "    bottom = plt.subplot2grid((3, 2), (2, 0), colspan=2)\n",
    "    \n",
    "    # Plot 2d sensitivity curves\n",
    "    #Sensitivity.heatmap(subset[['temperature', 'vsini', 'detrate']], ax=left, **kwargs)\n",
    "    #Sensitivity.heatmap(subset[['temperature', 'vsini', 'diff']], ax=right, **kwargs)\n",
    "    sns.heatmap(subset.pivot('temperature', 'vsini', 'detrate'), ax=left, annot=True, **kwargs)\n",
    "    sns.heatmap(subset.pivot('temperature', 'vsini', 'diff'), ax=right, annot=True, **kwargs)\n",
    "    left.set_title('Actual Sensitivity')\n",
    "    right.set_title('Residuals')\n",
    "    \n",
    "    # Plot residual histogram\n",
    "    residuals = subset.loc[subset['diff'].notnull(), 'diff'].values\n",
    "    hist(residuals, bins=30)\n",
    "    #bottom.set_yscale('log')\n",
    "    #ylim = bottom.get_ylim()\n",
    "    #bottom.set_ylim((0.5, ylim[1]))\n",
    "    bottom.set_xlabel('Detection Rate Residuals')\n",
    "    bottom.set_ylabel('Number')\n",
    "    return \n",
    "\n",
    "star = 'HIP 93580'\n",
    "date = summary.loc[summary.star == star].date.unique()[0]\n",
    "residuals = plot_residuals(star, date, pipe, features, cmap=viridis, refit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the completeness curves look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_detrate(T2_vals, T1=10000, v1sini=100, v2sini=10.0, snr=500):\n",
    "    T1_vals = np.ones_like(T2_vals) * T1\n",
    "    v1sini_vals = np.ones_like(T2_vals) * v1sini\n",
    "    v2sini_vals = np.ones_like(T2_vals) * v2sini\n",
    "    snr_vals = np.ones_like(T2_vals) * snr\n",
    "    X_tmp = np.vstack((T2_vals, T1_vals, v2sini_vals, v1sini_vals, snr_vals)).T\n",
    "    y_tmp = pipe.predict_proba(X_tmp)[:, 1]\n",
    "    return y_tmp\n",
    "\n",
    "T2_vals = np.arange(3000, 12000, 10)\n",
    "y1 = get_detrate(T2_vals, v2sini=0)\n",
    "y2 = get_detrate(T2_vals, v2sini=50)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,7))\n",
    "plt.plot(T2_vals, y1, label='S/N = 500')\n",
    "plt.plot(T2_vals, y2, label='S/N = 100')\n",
    "plt.legend(loc='best', fancybox=True)\n",
    "plt.xlabel('Secondary Temperature')\n",
    "plt.ylabel('Predicted Detection Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusions:\n",
    "It definitely gets the shape of the curve right, and reacts to all the parameters in the right way. The only issue is doesn't too very well in the transition region from detection rate = 1 --> 0. Moving on to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@models.custom_model\n",
    "def sigmoid(x, alpha=-0.2, beta=20):\n",
    "    f = alpha*(x-beta)\n",
    "    return 1.0/(1+np.exp(-f))\n",
    "\n",
    "def fit_sigmoid(x, y):\n",
    "    m_init = sigmoid()\n",
    "    f = fitting.LevMarLSQFitter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('error')\n",
    "        m = f(m_init, x, y, maxiter=10000)\n",
    "    return m\n",
    "\n",
    "#from itertools import cycle\n",
    "#colors = cycle(('red', 'blue', 'green', 'black'))\n",
    "def get_data(star, date, addmode='simple'):\n",
    "    subset = summary.loc[(summary.star == star) & \n",
    "                         (summary.date == date) & \n",
    "                         (summary.addmode == addmode)].dropna(subset=('detrate',))\n",
    "    \n",
    "    # Add some points at low vsini and high temperature (always detected)\n",
    "    highT = np.arange(7000, 12100, 200)\n",
    "    tmp = pd.DataFrame(data=dict(temperature=highT, detrate=np.ones_like(highT)))\n",
    "    for vsini in [0, 10, 20, 30, 40, 50]:\n",
    "        idx = np.argmax(subset.loc[subset.vsini == vsini, 'temperature'])\n",
    "        if subset.loc[idx, 'detrate'] == 1.0:\n",
    "            tmp['vsini'] = vsini\n",
    "            subset = pd.concat((subset, tmp))\n",
    "    \n",
    "    # Now, add some points at high vsini and low temperature (never detected)\n",
    "    high_vsini = np.arange(75, 250+25, 25)\n",
    "    tmp = pd.DataFrame(data=dict(vsini=high_vsini, detrate=np.zeros_like(high_vsini)))\n",
    "    for teff in range(3000, 7000, 100):\n",
    "        idx = np.argmax(subset.loc[subset.temperature == teff, 'vsini'])\n",
    "        if subset.loc[idx, 'detrate'] == 0.0:\n",
    "            tmp['temperature'] = teff\n",
    "            tmp['detrate'] = 0.0\n",
    "            subset = pd.concat((subset, tmp))\n",
    "        elif subset.loc[idx, 'detrate'] < 0.5:\n",
    "            vsini = subset.loc[subset.temperature == teff, 'vsini']\n",
    "            detrate = subset.loc[subset.temperature == teff, 'detrate']\n",
    "            try:\n",
    "                sigmoid_fcn = fit_sigmoid(vsini.values, detrate.values)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            tmp['temperature'] = teff\n",
    "            tmp['detrate'] = sigmoid_fcn(tmp['vsini'])\n",
    "            subset = pd.concat((subset, tmp))\n",
    "            #col = colors.next()\n",
    "            #plt.scatter(vsini, detrate, c=col)\n",
    "            #plt.plot(vsini, sigmoid_fcn(vsini), color=col)\n",
    "        else:\n",
    "            break\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test out on a random star\n",
    "star = summary.star.unique()[1]\n",
    "date = summary.loc[summary.star == star, 'date'].unique()[0]\n",
    "subset = get_data(star=star, date=date)\n",
    "\n",
    "#subset = subset.loc[subset.temperature < 7000]\n",
    "X = subset[['temperature', 'vsini']].as_matrix()\n",
    "y = subset['detrate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(3000, 12000, num=200)\n",
    "yy = np.linspace(0, 250, num=200)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "test_pts = np.array((XX, YY)).reshape(2, -1).T\n",
    "\n",
    "# Interpolate\n",
    "s = CloughTocher2DInterpolator(X, y, rescale=True)\n",
    "mu = s(test_pts[:, 0], test_pts[:, 1])\n",
    "\n",
    "# Plot\n",
    "fig, (ax) = plt.subplots(1, 1, figsize=(14, 7))\n",
    "im = ax.scatter(test_pts[:, 0], test_pts[:, 1], c=mu, s=50, marker='s', edgecolor='none', \n",
    "                        cmap=viridis, vmin=0, vmax=1)\n",
    "im = ax.scatter(X[:, 0], X[:, 1], c=y, s=50, marker='s', cmap=viridis, vmin=0, vmax=1)\n",
    "cax = plt.colorbar(im, ax=ax)\n",
    "ax.set_xlim((2950, 12050))\n",
    "ax.set_ylim((-3, 253))\n",
    "ax.set_xlabel('Temperature (K)')\n",
    "ax.set_ylabel('vsini (km/s)')\n",
    "cax.set_label('Completeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpolate the completeness for every star.\n",
    "output_dir = 'Completeness'\n",
    "for star in summary.star.unique():\n",
    "    for date in summary.loc[summary.star == star, 'date'].unique():\n",
    "        # Get the data (supplemented with some assumption about the shape of the surface)\n",
    "        subset = get_data(star=star, date=date)\n",
    "        X = subset[['temperature', 'vsini']].as_matrix()\n",
    "        y = subset['detrate'].values\n",
    "        \n",
    "        # Interpolate\n",
    "        s = CloughTocher2DInterpolator(X, y, rescale=True)\n",
    "        \n",
    "        # Calculate the completeness on a large grid\n",
    "        xx = np.linspace(3000, 12000, num=200)\n",
    "        yy = np.linspace(0, 250, num=200)\n",
    "        XX, YY = np.meshgrid(xx, yy)\n",
    "        test_pts = np.array((XX, YY)).reshape(2, -1).T\n",
    "        mu = s(test_pts[:, 0], test_pts[:, 1])\n",
    "\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
    "        im = ax.scatter(test_pts[:, 0], test_pts[:, 1], c=mu, s=50, marker='s', edgecolor='none', \n",
    "                                cmap=viridis, vmin=0, vmax=1)\n",
    "        im = ax.scatter(X[:, 0], X[:, 1], c=y, s=50, marker='s', cmap=viridis, vmin=0, vmax=1)\n",
    "        cax = plt.colorbar(im, ax=ax)\n",
    "        ax.set_xlim((2950, 12050))\n",
    "        ax.set_ylim((-3, 253))\n",
    "        ax.set_xlabel('Temperature (K)')\n",
    "        ax.set_ylabel('vsini (km/s)')\n",
    "        cax.set_label('Completeness')\n",
    "        \n",
    "        # Save the interpolator and the figure\n",
    "        fig.savefig('{}/{}_{}.pdf'.format(output_dir, \n",
    "                                          star.replace(' ', '_'), \n",
    "                                          date.replace('-', '')))\n",
    "        plt.close('all')\n",
    "        with open('{}/{}_{}.pkl'.format(output_dir, \n",
    "                                        star.replace(' ', '_'), \n",
    "                                        date.replace('-', '')), 'w') as outfile:\n",
    "            pickle.dump(s, outfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
